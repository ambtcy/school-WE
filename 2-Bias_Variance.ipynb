{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5497ebb",
   "metadata": {},
   "source": [
    "#  Bias & Variance\n",
    "\n",
    "When fitting a model to data there can be three types of errors, Noise, Bias and Variance.\n",
    "\n",
    "* Noise is random and cannot be reduced (irreducible error)\n",
    "* in contract bias and variance are reducible errors:\n",
    " * Bias represents a constant offset from the true value, so is linked to the concept of accuracy,\n",
    " * Variance represents a spread in the modelled values, so is linked to the concept of Precision.\n",
    "\n",
    "If the model cannot fit the data exactly we can end up in a situation where a model is either overfitted or underfitted.\n",
    "To assess a model, we need to fit (train) a it on one set of data and then test it on another unseen set of data. \n",
    "If the model is overfitted, it has been pushed to get an exact match (e.g. minimise the SSE) at every point, this leads to a low bias error. However when we apply the model to new unseen data it is often a poor fit, as it has tried to model the random noise in the training data. The error is therefore high on this test data and it has a high variance.\n",
    "\n",
    "Conversely if the model is underfitted then it may have a poor fit (e.g. high SSE) on the training data, so a high bias, but an equally poor fit on the test data, so the variance is low.\n",
    "\n",
    "This is best demonstrated visuallty, and the following example aims to demonstrate over and under-fitting a model, and to demonstrate the bias/variance trade-off, where we need to get the best compromised.\n",
    "\n",
    "In this example we are modelling a cosine function using a polynomial fit. The degree of the polynomial is incremented from 1 to 15. \n",
    "\n",
    "The degree of the polynomial here is the model's hyperparameter. \n",
    "\n",
    "The Root Mean Squared Error (RMSE) is calculated for each model for the training data and an independent test data set to assess the goodness of fit for the model.\n",
    "\n",
    "#### Pseudocode:\n",
    "<code>    X = generate N random points\n",
    "    y = apply cosine to X and add random noise\n",
    "    split X and y in half\n",
    "    for degree of polynomial from 1 to 15:\n",
    "        train polynomial model on training data\n",
    "        plot model vs expected for given degree of polynomial\n",
    "        calculate RMSE on training data\n",
    "        calculate RMSE on test data\n",
    "    plot training and test RMSE against degree of polynomial\n",
    "</code>\n",
    "\n",
    "#### Exercises:\n",
    "1. What order of polynomial do you think gives the \"best fit\"\n",
    "2. Experiment with changing the frequency of the cosine (parameter <code>B</code>) in the input model.\n",
    "3. Experiment with changing the size (<code>n_samples</code>) of the data set and test/train <code>split</code> on the error plots.\n",
    "4. Experiment with increasing the irreducible error (<code>noise</code>) variable.\n",
    "5. Extend the true_fun to a more complex relationship e.g. (<code>A * np.cos(B * np.pi * X + C) + A/2 * np.cos(2*B * np.pi * X + C)</code>\n",
    "\n",
    "#### Maths\n",
    "$$RMSE=\\sqrt{\\dfrac{\\sum_{i=1}^{n}{(f(x_i) - y_i)^2}}{n}}$$\n",
    "\n",
    "Input equation, here $\\epsilon$ is random noise (technically normally distributed noise, there are other types of noise!)\n",
    "$$y = A\\cos{(B\\pi x + C)} + \\epsilon$$\n",
    "\n",
    "\n",
    "Model used for fitting.\n",
    "$$f(x) = a_0 + a_1x + a_2x^2 + ... + a_{n-1}x^{n-1}+ a_nx^n$$\n",
    "\n",
    "This example is adapted from <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\">Scikit-learn examples: Underfitting vs. Overfitting</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fcba2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the \"true\" function\n",
    "def true_fun(X, A=1, B=1.5, C=0):\n",
    "    return A * np.cos(B * np.pi * X + C)\n",
    "\n",
    "# random seed - ensures we have same model, change as desired\n",
    "np.random.seed(0)\n",
    "\n",
    "# define the size of the dataset - grow or shrink\n",
    "n_samples = 60\n",
    "\n",
    "# how to partition the test/train data set, 0.5 = 50% split.\n",
    "split = 0.5 \n",
    "\n",
    "# noise weight\n",
    "noise = 0.1\n",
    "\n",
    "# define the polynomial degrees to work through\n",
    "degrees = range(1,16,1)\n",
    "  \n",
    "    \n",
    "\n",
    "# create dataset and split\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * noise\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=split, random_state=None\n",
    ")\n",
    "\n",
    "# setup subplot grids\n",
    "subplot_width = 5\n",
    "subplot_height = 1+int(np.ceil(len(degrees)/subplot_width))\n",
    "plt.figure(figsize=(5*subplot_width, 5*subplot_height))\n",
    "\n",
    "# setup arrays to store the errors\n",
    "training_error = np.zeros(len(degrees))\n",
    "test_error = np.zeros(len(degrees))\n",
    "\n",
    "# iterate over polynomial degrees\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(subplot_height, subplot_width, i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    # create a model\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression),\n",
    "        ]\n",
    "    )\n",
    "    pipeline.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "    # create subplot of fit for each polynomial degree\n",
    "    X_show = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_show, pipeline.predict(X_show[:, np.newaxis]),color='firebrick',linewidth=3, label=\"Model\")\n",
    "    plt.plot(X_show, true_fun(X_show),color='darkgrey',linestyle='dashed', label=\"True function\")\n",
    "    plt.scatter(X_train, y_train, marker='o',color=\"black\", s=20, label=\"Train samples\")\n",
    "    plt.scatter(X_test, y_test, marker='o',color=\"black\",facecolors='none', s=20, label=\"Test samples\")\n",
    "    # formatting for plot\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\".format(degrees[i]))\n",
    "\n",
    "    # calculate the Root Mean Squared Error for a given point.\n",
    "    np.put(training_error,i,\n",
    "           mean_squared_error(y_train, pipeline.predict(X_train[:, np.newaxis]),squared=True)\n",
    "          )\n",
    "    np.put(test_error,i, \n",
    "           mean_squared_error(y_test, pipeline.predict(X_test[:, np.newaxis]),squared=True)\n",
    "          )\n",
    "    \n",
    "# show grid of fits\n",
    "plt.show()\n",
    "\n",
    "# Show plot of training vs test errors\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(degrees,training_error,color='steelblue',linewidth=3, label='Training Error')\n",
    "plt.plot(degrees,test_error,color='firebrick',linewidth=3,label='Test Error')\n",
    "plt.xlabel(\"Degree of polynomial\")\n",
    "plt.ylabel(\"RMS Error\")\n",
    "plt.yscale('log')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Root Mean Squared Error for training and test data, for polynomial fit to cosine function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14286f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af77cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2023)",
   "language": "python",
   "name": "python3-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
