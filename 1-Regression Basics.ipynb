{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a96664",
   "metadata": {},
   "source": [
    "##  Line of Best Fit Examples\n",
    "The following aims to demonstrate how we can determine a line of best fit.\n",
    "\n",
    "We start with a simple relationship:\n",
    "$$ y = f(x; A, B) = A + Bx $$\n",
    "This is the equation of the straigt line, which intercepts the y-axis at $A$ and has a slope/gradient of $B$.\n",
    "\n",
    "We use this relationship to generate some data by applying the function and adding some noise ($\\epsilon$), i.e.\n",
    "$$y = A + Bx + \\epsilon $$ \n",
    "\n",
    "In the cell below complete the function f to return the value of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "    Write a real Function to evaluate y = f(x; A, B) = A + Bx\n",
    "'''\n",
    "def f(x, A, B):\n",
    "    y=0\n",
    "    ## TODO    \n",
    "    return y\n",
    "'''\n",
    "    The below prints the values\n",
    "'''\n",
    "\n",
    "# Real values for equations.\n",
    "A_true=5\n",
    "B_true=3\n",
    "\n",
    "# amount of noise to add\n",
    "error_sigma = 10\n",
    "\n",
    "# number of data points\n",
    "N = 100\n",
    "# range of x values to use.\n",
    "x_start = -9\n",
    "x_stop = 9\n",
    "\n",
    "# setup xs and ys and a noise parameter\n",
    "noise = np.random.normal(0,error_sigma,N)\n",
    "xs = np.linspace(x_start,x_stop,N)\n",
    "ys = f(xs,A_true,B_true) + noise\n",
    "\n",
    "# plot x,y with best fit\n",
    "plt.scatter(xs,ys, label=\"data\")\n",
    "plt.plot(xs,f(xs,A_true,B_true), color=\"black\", linestyle=\"--\", label=\"underlying\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb67fb9",
   "metadata": {},
   "source": [
    "## Defining Error\n",
    "A new line can be plotted through the data using any value of $A$ and $B$ to calculate a set of predicted y values ($y_{pred}$)\n",
    "\n",
    "How well this line fits the data can be determined calculating an error between the data ($y_{real}$) and $y_{pred}$ as $x$ is the same, i.e.\n",
    "$$error =  y_{real}  - y_{pred}$$\n",
    "\n",
    "The sum of the errors is then a measure of goodness of fit.\n",
    "\n",
    "The error however can be both positive and negative, which can cause problems, e.g. if we had values with a large differnces above and below the line they would cancel out and make the sum error small.\n",
    "\n",
    "To avoid this we square the error before summing:\n",
    "\n",
    "$$SSE = \\sum{(y_{real}  - y_{pred})^2}$$\n",
    "\n",
    "Where SSE stands for sum of square error.\n",
    "Sometimes we use the Mean Square Error ($MSE$) as:\n",
    "$$MSE = \\sqrt{\\sum{(y_{real}  - y_{pred})^2}}$$\n",
    "\n",
    "In the cell below complete the function to calculate $SSE$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd094c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Write a function to calculate the Square Error function\n",
    "    y_pred the predicted y values\n",
    "    y_real the real (data)\n",
    "'''\n",
    "def sum_square_error(y_pred, y_real):\n",
    "    SSE=0\n",
    "    ## TODO\n",
    "    return SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ccf4f3",
   "metadata": {},
   "source": [
    "One approach to determine the minimal value of $SSE$ is to systematically try all possible combinations of $A$ and $B$. This is known as a grid search. \n",
    "\n",
    "Grid searches are useful as they graphically show the pattern of how variables are related, however they require lots of calculations.\n",
    "\n",
    "The cell below is setup to display a contour plot of a grid search..\n",
    "\n",
    "Three arrays are required to be populated, the start as:\n",
    "$As$ and $Bs$ are 1 x M arrays i.e. if M = 5\n",
    "$$As=[\\begin{array}{}\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{array}] $$\n",
    "$$Bs=[\\begin{array}{}\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{array}]$$\n",
    "\n",
    "The final array $SSEs$ is then M x M:\n",
    "\n",
    "$$\n",
    "SSEs = \\left[\n",
    "\\begin{array}{}\n",
    "0 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d84ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid\n",
    "## number of values to test for each parameter\n",
    "M=50\n",
    "## ranges to test over\n",
    "As = np.linspace(A_true*0.5, A_true*1.5 , M)\n",
    "Bs = np.linspace(B_true*0.5, B_true*1.5 , M)\n",
    "\n",
    "## empty array of the correct size for SSE values\n",
    "SSEs = np.empty((M,M))\n",
    "\n",
    "''' \n",
    "    Write code here to iterate over As and Bs and calculate SSE to fill in the SSEs array.\n",
    "    You may find the zip function helpful.\n",
    "'''\n",
    "\n",
    "## TODO\n",
    "\n",
    "'''\n",
    "    code below will display contours from grid\n",
    "'''        \n",
    "# find minima from grid search\n",
    "i, j = np.where(SSEs==np.min(SSEs))\n",
    "A_min=As[j]\n",
    "B_min=Bs[i]\n",
    " \n",
    "# plot grid\n",
    "ax = plt.gca()     \n",
    "ax.contour(As,Bs,SSEs,levels=50)\n",
    "ax.plot(A_min,B_min, color=\"red\", marker = \"o\", zorder = 10, markersize=4, clip_on=False)\n",
    "plt.xlabel(\"A\")\n",
    "plt.ylabel(\"B\")\n",
    "plt.show()\n",
    "\n",
    "# plot x,y with best fit\n",
    "print(\"A = {}, B = {}\".format(A_min,B_min))\n",
    "plt.scatter(xs,ys, label=\"data\")\n",
    "plt.plot(xs,f(xs,A_min,B_min), color=\"red\", label=\"best fit\")\n",
    "plt.plot(xs,f(xs,A_true,B_true), color=\"darkgrey\", linestyle=\"--\", label=\"underlying\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c3125",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e608d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# initial guess\n",
    "A_1 = np.min(As)\n",
    "B_1 = np.min(Bs)\n",
    "\n",
    "# lists for capturing the params and SSE from each \"step\" (iteration)\n",
    "param_steps=[]\n",
    "SSE_steps=[]\n",
    "\n",
    "'''\n",
    "    Function to calculate get SSE by applying the function using A and B\n",
    "'''\n",
    "def apply_and_calc_SSE(params):\n",
    "    # setup A and B from the array\n",
    "    A = params[0]\n",
    "    B = params[1]\n",
    "    # calculate error (SSE)\n",
    "    err= sum_square_error(ys,f(xs,A,B))\n",
    "    # update lists\n",
    "    param_steps.append(params)\n",
    "    SSE_steps.append(err)\n",
    "    return err\n",
    "\n",
    "# Perform optimisation using minimse.\n",
    "res = minimize(apply_and_calc_SSE, (A_1, B_1), method=\"Nelder-Mead\")\n",
    "print(\"Miminised Parameters A = {}, B = {}\".format(res.x[0],res.x[1]))\n",
    "print(\"Iterations until stable {}\".format(res.nit))\n",
    "\n",
    "param_steps=np.stack(np.array(param_steps))\n",
    "ax = plt.gca()     \n",
    "ax.contour(As,Bs,SSEs,levels=50)\n",
    "ax.plot(param_steps[:,0],param_steps[:,1],color=\"blue\",linestyle=\"--\",zorder = 10)\n",
    "ax.plot(A_min,B_min, color=\"red\", marker = \"o\", zorder = 10, markersize=4, clip_on=False)\n",
    "plt.xlabel(\"A\")\n",
    "plt.ylabel(\"B\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(SSE_steps)),SSE_steps)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27309fb",
   "metadata": {},
   "source": [
    "## Exact Solution\n",
    "For \"simple\" linear cases, an exact solution can be found. This proof\\[1\\] requires calculus, as derivitives are required to find minimia - so this proof is above KS4 maths.\n",
    "\n",
    "The result however is simple to calculate albeit arduous to apply by hand!\n",
    "\n",
    "For all values of x and y calculate the mean ($\\bar{x}, \\bar{y}$) values via:\n",
    "$$\\bar{x}=\\frac{\\sum{x}}{n}, \\bar{y}=\\frac{\\sum{y}}{n}$$\n",
    "\n",
    "Then calculate the difference ($\\Delta{x}, \\Delta{y}$) between each point at the mean as\n",
    "$$ \\Delta{x} = (x - \\bar{x}), \\Delta{y} = (y - \\bar{y})$$\n",
    "\n",
    "\n",
    "The value of $\\hat{B}$ can then be found as:\n",
    "$$ \\hat{B} = \\frac{\\sum{(x - \\bar{x})(y - \\bar{y})}}{\\sum{(x - \\bar{x})^2}}=\\frac{\\sum{\\Delta{x}\\Delta{y}}}{\\sum{\\Delta{x}^2}}=\\frac{s_{xy}}{s^2_x}$$\n",
    "\n",
    "and this can then be used to find $\\hat{A}$ as:\n",
    "$$\\hat{A} = \\bar{y} - \\hat{B}\\bar{x}$$\n",
    "\n",
    "Note: we use $\\hat{A}$ & $\\hat{B}$ rather than $A$ & $B$ as they are estimates of the underlying values given the data.\n",
    "\n",
    "As you can see by rerunning the above cells, the minimsed values of $A$ & $B$ will change based on the random noise introduced, even if the true values set at the top remain the same.\n",
    "\n",
    "Write some code int he cell below to calculate $\\hat{A}$ and $\\hat{B}$\n",
    "\n",
    "Reference:\n",
    "1. https://statproofbook.github.io/P/slr-ols.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d159119",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(xs)\n",
    "y_mean = np.mean(ys)\n",
    "delta_x = xs-x_mean\n",
    "delta_y = ys-y_mean\n",
    "\n",
    "SXY = np.sum(delta_x * delta_y)\n",
    "SXX = np.sum(delta_x * delta_x)\n",
    "B = SXY/SXX\n",
    "A = y_mean  - B *x_mean \n",
    "\n",
    "\n",
    "'''\n",
    "Plot graph of values - expect A and B to be calculated above.\n",
    "'''\n",
    "print(\"A = {}, B = {}\".format(A,B))\n",
    "# plot x,y with best fit\n",
    "plt.scatter(xs,ys)\n",
    "plt.plot(xs,f(xs,A,B), color=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3f5b2",
   "metadata": {},
   "source": [
    "This process is known and Ordinary Least Squares (OLS), there are numerous libraries that apply this, the below code uses statsmodels, check the values of A and B vs you're calculation. \n",
    "\n",
    "The OLS library also provides \"goodness of fit\" metrics such as R-Squared, the closer this is to 1 the better the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd241ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "print(sm.__version__)\n",
    "X = xs\n",
    "X = sm.add_constant(X)\n",
    "result = sm.OLS(ys,X).fit()\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945ea69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2021)",
   "language": "python",
   "name": "python3-2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
