{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.8.20","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"43a96664","cell_type":"markdown","source":"##  Line of Best Fit Examples\nThe following aims to demonstrate how we can determine a line of best fit.\n\nWe start with a simple relationship:\n$$ y = f(x; A, B) = A + Bx $$\nThis is the equation of the straigt line, which intercepts the y-axis at $A$ and has a slope/gradient of $B$.\n\nWe use this relationship to generate some data by applying the function and adding some noise ($\\epsilon$), i.e.\n$$y = A + Bx + \\epsilon $$ \n\nIn the cell below complete the function f to return the value of y.","metadata":{}},{"id":"c62cf598","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n'''\n    Write a real Function to evaluate y = f(x; A, B) = A + Bx\n'''\ndef f(x, A, B):\n    y=0\n    y = A + B*x\n    return y\n'''\n    The below prints the values\n'''\n\n# Real values for equations.\nA_true=5\nB_true=3\n\n# amount of noise to add\nerror_sigma = 10\n\n# number of data points\nN = 100\n# range of x values to use.\nx_start = -9\nx_stop = 9\n\n# setup xs and ys and a noise parameter\nnoise = np.random.normal(0,error_sigma,N)\nxs = np.linspace(x_start,x_stop,N)\nys = f(xs,A_true,B_true) + noise\n\n# plot x,y with best fit\nplt.scatter(xs,ys, label=\"data\")\nplt.plot(xs,f(xs,A_true,B_true), color=\"black\", linestyle=\"--\", label=\"underlying\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8cb67fb9","cell_type":"markdown","source":"## Defining Error\nA new line can be plotted through the data using any value of $A$ and $B$ to calculate a set of predicted y values ($y_{pred}$)\n\nHow well this line fits the data can be determined calculating an error between the data ($y_{real}$) and $y_{pred}$ as $x$ is the same, i.e.\n$$error =  y_{real}  - y_{pred}$$\n\nThe sum of the errors is then a measure of goodness of fit.\n\nThe error however can be both positive and negative, which can cause problems, e.g. if we had values with a large differnces above and below the line they would cancel out and make the sum error small.\n\nTo avoid this we square the error before summing:\n\n$$SSE = \\sum{(y_{real}  - y_{pred})^2}$$\n\nWhere SSE stands for sum of square error.\nSometimes we use the Mean Square Error ($MSE$) as:\n$$MSE = \\sqrt{\\sum{(y_{real}  - y_{pred})^2}}$$\n\nIn the cell below complete the function to calculate $SSE$.","metadata":{}},{"id":"dd094c53","cell_type":"code","source":"'''\n    Write a function to calculate the Square Error function\n    y_pred the predicted y values\n    y_real the real (data)\n'''\ndef sum_square_error(y_pred, y_real):\n    SSE=0\n    SSE = np.sum((y_pred-y_real)**2)\n    return SSE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91ccf4f3","cell_type":"markdown","source":"One approach to determine the minimal value of $SSE$ is to systematically try all possible combinations of $A$ and $B$. This is known as a grid search. \n\nGrid searches are useful as they graphically show the pattern of how variables are related, however they require lots of calculations.\n\nThe cell below is setup to display a contour plot of a grid search..\n\nThree arrays are required to be populated, the start as:\n$As$ and $Bs$ are 1 x M arrays i.e. if M = 5\n$$As=[\\begin{array}{}\n0 & 0 & 0 & 0 & 0\n\\end{array}] $$\n$$Bs=[\\begin{array}{}\n0 & 0 & 0 & 0 & 0\n\\end{array}]$$\n\nThe final array $SSEs$ is then M x M:\n\n$$\nSSEs = \\left[\n\\begin{array}{}\n0 & 0 & 0 & 0 & 0 \\\\ \n0 & 0 & 0 & 0 & 0 \\\\ \n0 & 0 & 0 & 0 & 0 \\\\ \n0 & 0 & 0 & 0 & 0 \\\\ \n0 & 0 & 0 & 0 & 0\n\\end{array}\n\\right]\n$$","metadata":{}},{"id":"73d84ac0","cell_type":"code","source":"# Create a grid\n## number of values to test for each parameter\nM=50\n## ranges to test over\nAs = np.linspace(A_true*0.5, A_true*1.5 , M)\nBs = np.linspace(B_true*0.5, B_true*1.5 , M)\n\n## empty array of the correct size for SSE values\nSSEs = np.empty((M,M))\n\n''' \n    Write code here to iterate over As and Bs and calculate SSE to fill in the SSEs array.\n    You may find the zip function helpful.\n'''\n\nfor i in range(len(As)):\n    for j in range(len(Bs)):\n        SSEs[j,i] = sum_square_error(ys,f(xs, As[i],Bs[j]))\n\n'''\n    code below will display contours from grid\n'''        \n# find minima from grid search\ni, j = np.where(SSEs==np.min(SSEs))\nA_min=As[j]\nB_min=Bs[i]\n \n# plot grid\nax = plt.gca()     \nax.contour(As,Bs,SSEs,levels=50)\nax.plot(A_min,B_min, color=\"red\", marker = \"o\", zorder = 10, markersize=4, clip_on=False)\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\nplt.show()\n\n# plot x,y with best fit\nprint(\"A = {}, B = {}\".format(A_min,B_min))\nplt.scatter(xs,ys, label=\"data\")\nplt.plot(xs,f(xs,A_min,B_min), color=\"red\", label=\"best fit\")\nplt.plot(xs,f(xs,A_true,B_true), color=\"darkgrey\", linestyle=\"--\", label=\"underlying\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4a3c3125","cell_type":"markdown","source":"### Minimisation Algorithms\nInstead of performing a grid search there are numerous algorithms that can find the minima of a function. These algorithms have varying performance depending on the function we are trying to minimise.\n\nRun the below cell to observe the Nelder-Mead method, the plot shows the path through A and B the optimiser takes, and the SSE value for each iteration.\n\nFind and change the algorithm type to each of the following:\n1. \"Nelder-Mead\"\n2. \"Powell\"\n3. \"trust-constr\" (Trust region constrained method)\n4. \"L-BFGS-B\" (Broyden Fletcher Goldfarb Shanno Method, L for limited memory, B for bounds)\n\nObserve the path to 0 for each.\n\nReference:\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html","metadata":{}},{"id":"4e608d5b","cell_type":"code","source":"from scipy.optimize import minimize\n\n# initial guess\nA_1 = np.min(As)\nB_1 = np.min(Bs)\n\n# lists for capturing the params and SSE from each \"step\" (iteration)\nparam_steps=[]\nSSE_steps=[]\n\n'''\n    Function to calculate get SSE by applying the function using A and B\n'''\ndef apply_and_calc_SSE(params):\n    # setup A and B from the array\n    A = params[0]\n    B = params[1]\n    # calculate error (SSE)\n    err= sum_square_error(ys,f(xs,A,B))\n    # update lists\n    param_steps.append(params)\n    SSE_steps.append(err)\n    return err\n\n# Perform optimisation using minimse.\nres = minimize(apply_and_calc_SSE, (A_1, B_1), method=\"Nelder-Mead\")\nprint(\"Miminised Parameters A = {}, B = {}\".format(res.x[0],res.x[1]))\nprint(\"Iterations until stable {}\".format(res.nit))\n\nparam_steps=np.stack(np.array(param_steps))\nax = plt.gca()     \nax.contour(As,Bs,SSEs,levels=50)\nax.plot(param_steps[:,0],param_steps[:,1],color=\"blue\",linestyle=\"--\",zorder = 10)\nax.plot(A_min,B_min, color=\"red\", marker = \"o\", zorder = 10, markersize=4, clip_on=False)\nplt.xlabel(\"A\")\nplt.ylabel(\"B\")\nplt.show()\n\nplt.plot(SSE_steps)\nplt.xlabel(\"step\")\nplt.ylabel(\"SSE\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a27309fb","cell_type":"markdown","source":"## Exact Solution\nFor \"simple\" linear cases, an exact solution can be found. This proof\\[1\\] requires calculus, as derivitives are required to find minimia - so this proof is above KS4 maths.\n\nThe result however is simple to calculate albeit arduous to apply by hand!\n\nFor all values of x and y calculate the mean ($\\bar{x}, \\bar{y}$) values via:\n$$\\bar{x}=\\frac{\\sum{x}}{n}, \\bar{y}=\\frac{\\sum{y}}{n}$$\n\nThen calculate the difference ($\\Delta{x}, \\Delta{y}$) between each point at the mean as\n$$ \\Delta{x} = (x - \\bar{x}), \\Delta{y} = (y - \\bar{y})$$\n\n\nThe value of $\\hat{B}$ can then be found as:\n$$ \\hat{B} = \\frac{\\sum{(x - \\bar{x})(y - \\bar{y})}}{\\sum{(x - \\bar{x})^2}}=\\frac{\\sum{\\Delta{x}\\Delta{y}}}{\\sum{\\Delta{x}^2}}=\\frac{s_{xy}}{s^2_x}$$\n\nand this can then be used to find $\\hat{A}$ as:\n$$\\hat{A} = \\bar{y} - \\hat{B}\\bar{x}$$\n\nNote: we use $\\hat{A}$ & $\\hat{B}$ rather than $A$ & $B$ as they are estimates of the underlying values given the data.\n\nAs you can see by rerunning the above cells, the minimsed values of $A$ & $B$ will change based on the random noise introduced, even if the true values set at the top remain the same.\n\nWrite some code in the cell below to calculate $\\hat{A}$ and $\\hat{B}$\n\nReference:\n1. https://statproofbook.github.io/P/slr-ols.html\n","metadata":{}},{"id":"7d159119","cell_type":"code","source":"'''\n    Calculation of A and B via manual method.\n'''\n\ndx = (xs-np.mean(xs))\ndy = (ys-np.mean(ys))\nB = np.sum(dx*dy)/np.sum(dx**2)\nA = np.mean(ys)-B * np.mean(xs)\n\n'''\nPlot graph of values - expect A and B to be calculated above.\n'''\nprint(\"A = {}, B = {}\".format(A,B))\n# plot x,y with best fit\nplt.scatter(xs,ys)\nplt.plot(xs,f(xs,A,B), color=\"red\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"20e3f5b2","cell_type":"markdown","source":"This process is known and Ordinary Least Squares (OLS), there are numerous libraries that apply this, the below code uses statsmodels, check the values of A and B vs you're calculation. \n\nThe OLS library also provides \"goodness of fit\" metrics such as R-Squared, the closer this is to 1 the better the fit.","metadata":{}},{"id":"5cd241ad","cell_type":"code","source":"import statsmodels.api as sm \nprint(sm.__version__)\nX = xs\nX = sm.add_constant(X)\nresult = sm.OLS(ys,X).fit()\nprint(result.summary())\n\n# plot x,y with underlying trend\nplt.scatter(xs,ys, label=\"data\")\nplt.plot(xs,f(xs,A_true,B_true), color=\"black\", linestyle=\"--\", label=\"underlying\")\nplt.plot(xs,f(xs,result.params[0],result.params[1]), color=\"red\", linestyle=\"-\", label=\"Fit\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"01224d3f","cell_type":"markdown","source":"## Extension to Polynomials\n\nQuadratic Equations should be familiar at GCSE, these are equations in which the x value is squared.\n$$ y = B_0 + B_1 \\cdot x + B_2 \\cdot x^2 $$\n\nThis can be extended indefinitly with higher powers as:\n$$ y = B_0 + B_1 \\cdot x + B_2 \\cdot x^2 + B_3 \\cdot x^3 + \\space ... \\space + B_n \\cdot x^n $$\n\nThis is known as a polynomial equation. Quadratic equations are known as second order polynomials as the highest power is 2.\n\nIn the below follow how the function poly() works, and then increase B to simulate higher order polynomials.","metadata":{}},{"id":"065f5cd8","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n'''\n    Polynomial equation.\n'''\ndef poly(x, B):\n    y = 0\n    for i in range(len(B)):\n        y = y + (B[i] * (x**i))\n    return y\n'''\n    The below prints the values\n'''\n\n# Real values for equations.\nB = [5, 4 , -2]\n\n# amount of noise to add\nerror_sigma = 30\n\n# number of data points\nN = 100\n# range of x values to use.\nx_start = -9\nx_stop = 9\n\n# setup xs and ys and a noise parameter\nnoise = np.random.normal(0,error_sigma,N)\nxs = np.linspace(x_start,x_stop,N)\nys = poly(xs, B) + noise\n\n# plot x,y with underlying trend\nplt.scatter(xs,ys, label=\"data\")\nplt.plot(xs,poly(xs,B), color=\"black\", linestyle=\"--\", label=\"underlying\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"601e1a86","cell_type":"markdown","source":"OLS regerssion can also be applied to polynomials, however we need to change the input array to the statsmodel to use an array \n\n$$ X = \\left[\n\\begin{array}{}\n0 & x_1 & x_1^2 & x_1^3 & \\dots & x_1^n \\\\\n0 & x_2 & x_2^2 & x_2^3 & \\dots & x_2^n \\\\\n\\vdots &\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & x_m & x_1^m & x_m^3 & \\dots & x_m^n \n\\end{array}{}\n\\right]$$\n\nWe can use a library to do this transformation for us as below, and then use the same methods as before.","metadata":{}},{"id":"b37d9b6c","cell_type":"code","source":"import statsmodels.api as sm \nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_features= PolynomialFeatures(degree=len(B))\nX = polynomial_features.fit_transform(xs.reshape(-1,1))\nprint(X.shape)\nresult = sm.OLS(ys,X).fit()\nprint(result.summary())\n\n# plot x,y with underlying trend\nplt.scatter(xs,ys, label=\"data\")\nplt.plot(xs,poly(xs,B), color=\"black\", linestyle=\"--\", label=\"underlying\")\nplt.plot(xs,poly(xs,result.params), color=\"red\", linestyle=\"-\", label=\"Fit\")\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"299e9ba2","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}