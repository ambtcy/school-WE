{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9d6899",
   "metadata": {},
   "source": [
    "# Data pre-processing with SVM basic examples\n",
    "\n",
    "### Pre-processing\n",
    "Several steps can be taken to process data before it is modelled, this can include:\n",
    "* scaling and \"standardising\" the data, to ensure all input parameters have equal weight, \n",
    "* reducing the number of input variables by removing those that provide complimentary information (e.g. Principal Component Analysis - PCA), \n",
    "* transforming the data to a space where the modelling step is easier.\n",
    "\n",
    "This is known as data processing or pre-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0e5aa",
   "metadata": {},
   "source": [
    "### Linearisation\n",
    "One of the simplest examples of transforming data is linearisation. For equations where the x value is in the exponent, we can achieve a linear equation via logarithms (which are on some GCSE syllabuses!)\n",
    "logarithms are the inverse of an exponentationl operation i.e.\n",
    "$$ y = A^x $$\n",
    "$$ \\log_A{A^x} = x $$\n",
    "\n",
    "Therefore if we have an equation:\n",
    "$$ y = A^{2x+3} $$ \n",
    "We can take a log to give:\n",
    "$$\n",
    "\\log{y} = (2x+3)\\log{A}\\\\\n",
    "\\log{y} = (2\\log{A})x+(3\\log{A})\n",
    "$$\n",
    "This is a linear form ($y = B_0 + B_1x$)\n",
    "\n",
    "There is a special case where A = $e$ (Eulers number 2.71828)\n",
    "\n",
    "if x is in the denominator of an equation e.g.\n",
    "$$y = \\frac{A}{Bx}$$\n",
    "we can similarly linearise via logs as:\n",
    "$$ \\log{y} = \\log{\\frac{A}{Bx}}$$\n",
    "\n",
    "$$ \\log{y} = \\log{\\frac{A}{B}} - \\log{x}$$\n",
    "\n",
    "Finally if we have non-integer powers, we can use logs to linearise:\n",
    "\n",
    "$$ y = Ax^B $$ \n",
    "$$\\log{y}=B\\log{Ax}$$\n",
    "$$\\log{y}=B\\log{A} + B\\log{x}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aeb4f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# what is a log \n",
    "print(np.log2(2**16) == 16)\n",
    "print(np.log10(10**4.5) == 4.5)\n",
    "print(math.log(3**7,3) == 7) # note we use math here, as np doesn't have arbitary base.\n",
    "# log identities\n",
    "\n",
    "print(np.log(12.2/6.5))\n",
    "print(np.log(12.2) - np.log(6.5))\n",
    "print(np.log(5.6*8.2)) \n",
    "print(np.log(5.6) + np.log(8.2))\n",
    "M = 50\n",
    "error_sigma = .2\n",
    "xs = np.linspace(0.1,10,M)\n",
    "\n",
    "noise = np.random.normal(0,error_sigma,M)\n",
    "\n",
    "A = 2\n",
    "B = 2\n",
    "print(\"Exponential function: plot x vs log(y) becomes linear - fit with OLS\")\n",
    "plt.subplot(1, 2, 1)\n",
    "ys = np.exp((A*xs+B))\n",
    "plt.scatter(xs,ys)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(xs,np.log(ys))\n",
    "plt.show()\n",
    "\n",
    "print(\"Power equations - general case of above.\")\n",
    "plt.subplot(1, 2, 1)\n",
    "ys = A**(B*xs)\n",
    "plt.scatter(xs,ys)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(xs,np.log(ys))\n",
    "plt.show()\n",
    "\n",
    "print(\"Inverse equations - plot log(x) vs log(y) becomes linear - fit with OLS\")\n",
    "ys = A/(B*xs)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(xs,ys)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.log(xs),np.log(ys))\n",
    "plt.show()\n",
    "\n",
    "print(\"Non-integer powers plot log(x) vs log(y) becomes linear - fit with OLS\")\n",
    "ys = A*(xs**3.32) \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(xs,ys)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.log(xs),np.log(ys))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67534423",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVM) classify data by finding the points at the edge of classes and positioning a line equidistant between these points. \n",
    "\n",
    "They can account for overlapping classes, and work to minimise the errors by ignoring points.\n",
    "\n",
    "The below are simple 1D and then 2D examples, play with the errors on the points to observe how the classfication boundaries change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b098a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "N=20 # number of points\n",
    "error_sigma = 1\n",
    "good_centre = 5\n",
    "bad_centre = -5\n",
    "x=np.concatenate((np.random.normal(good_centre,error_sigma,N),np.random.normal(bad_centre,error_sigma,N))).reshape(-1, 1)\n",
    "labels = np.concatenate((np.full(N,1),np.full(N,0)))\n",
    "colors = np.concatenate((np.full(N,\"green\"),np.full(N,\"red\")))\n",
    "plt.scatter(x,np.zeros(2*N), color = colors)\n",
    "plt.show()\n",
    "\n",
    "model = SVC(kernel='linear', C=100)\n",
    "clf = model.fit(x,labels)\n",
    "xs = np.linspace(-10,10,500).reshape(-1,1)\n",
    "ys= clf.predict(xs)\n",
    "boundary=0\n",
    "for i in range(1,len(xs)):\n",
    "    if ys[i]!=ys[i-1]:\n",
    "        boundary = xs[i]\n",
    "    \n",
    "plt.scatter(x,np.zeros(2*N), color = colors)\n",
    "plt.vlines(clf.support_vectors_,-0.1,0.1, color=\"darkgrey\", linestyle=\"--\", label=\"Support vectors\")\n",
    "plt.vlines(boundary,-0.1,0.1, color=\"black\", linestyle=\"--\", label=\"boundary\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc09717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_plot(X,y,clf,x_axis_label=\"PC1\", y_axis_label=\"PC2\"):\n",
    "    \"\"\"\n",
    "        Helper to print the plots only ignore!\n",
    "        Based on answer provided by S. Loukas on StackOverflow.\n",
    "            https://stackoverflow.com/questions/51495819/how-to-plot-svm-decision-boundary-in-sklearn-python\n",
    "    \"\"\"\n",
    "    def make_meshgrid(x, y, h=.02):\n",
    "        x_min, x_max = x.min() - 1, x.max() + 1\n",
    "        y_min, y_max = y.min() - 1, y.max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return xx, yy\n",
    "\n",
    "    def plot_contours(ax, clf, xx, yy, **params):\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        out = ax.contourf(xx, yy, Z, **params)\n",
    "        return out\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # title for the plots\n",
    "    title = ('Decision surface of linear SVC ')\n",
    "\n",
    "    # Set-up grid for plotting.\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_ylabel(y_axis_label)\n",
    "    ax.set_xlabel(x_axis_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "'''\n",
    "    Start of example\n",
    "'''    \n",
    "N = 50\n",
    "error_sigma = 3\n",
    "good_centre = [5,3]\n",
    "bad_centre = [-5,-5]\n",
    "x1=np.concatenate((np.random.normal(good_centre[0],error_sigma,N),np.random.normal(bad_centre[0],error_sigma,N)))\n",
    "x2=np.concatenate((np.random.normal(good_centre[1],error_sigma,N),np.random.normal(bad_centre[1],error_sigma,N)))\n",
    "X = np.vstack((x1,x2)).T\n",
    "labels = np.concatenate((np.full(N,1),np.full(N,0)))\n",
    "colors = np.concatenate((np.full(N,\"green\"),np.full(N,\"red\")))\n",
    "plt.scatter(x1,x2, color = colors)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()\n",
    "\n",
    "model = SVC(kernel='linear', C=100)\n",
    "clf = model.fit(X,labels)\n",
    "decision_plot(X,labels,clf,\"X1\",\"X2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f27f9",
   "metadata": {},
   "source": [
    "### Body Temperature Example\n",
    "This is a simple body temperature example, where we have a data that is both higher and lower than the normal range.\n",
    "\n",
    "The data here is therefore not linearly separable. \n",
    "\n",
    "We standardise and then square to create a pseudo-dimension which transforms the problem into a linearly separable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20 # number of points in each zone, we have 4x this overall.\n",
    "\n",
    "# generate equal amounts of data:   34-35.8C (n = 20), 36-38 (n=40), 38.2-40 (n=20)\n",
    "t = np.r_[np.random.randint(340,358,N),np.random.randint(360,380,2*N), np.random.randint(382,400,N)]/10\n",
    "\n",
    "# label the data, 0 for ok, and 1 for outside healthy range.\n",
    "y = np.zeros(4*N)\n",
    "y = np.where(t<36,1,y)\n",
    "y = np.where(t>38,1,y)\n",
    "\n",
    "# print line as before -we have two zones, so can't use an SVM!\n",
    "colors = np.full(4*N,\"green\")\n",
    "colors = np.where(y==1,\"red\",colors)\n",
    "plt.scatter(t,np.zeros(4*N), color = colors)\n",
    "plt.show()\n",
    "\n",
    "# manual standardise\n",
    "z=(t-np.mean(t))/np.std(t,ddof=1)\n",
    "\n",
    "# add pseudo-dimension as a \"Kernel trick\"\n",
    "X=np.c_[z, z**2]\n",
    "\n",
    "# fit an SVM and plot, we set the regularisation parameter to 1\n",
    "model = SVC(kernel='linear', C=100)\n",
    "clf = model.fit(X,y)\n",
    "# plot to show the classification.\n",
    "decision_plot(X,y,clf,r\"$Temp_{std}$\",r\"$Temp^2_{std}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650b810",
   "metadata": {},
   "source": [
    "### Wine Dataset Example\n",
    "Here we use the wine dataset which we worked through in the slides and complete the example by using an SVM to split the data into 3 classes. The SVM is naturally a binary classify, so to make it multiclass we need to do either one-vs-one with the classes or one-vs-rest, the sckit-learn implementation is one-vs-one.\n",
    "\n",
    "This is based on:\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf82e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data and plit into test/train samples\n",
    "\n",
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "target_classes = range(0, 3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "for standardise in [False, True]:\n",
    "    # Optionally standardise data, we iterate over both to compare.\n",
    "    if standardise:\n",
    "        msg =\"With Standardisation\"\n",
    "        scaler = StandardScaler()#.set_output(transform=\"pandas\")\n",
    "        scaled_X_train = scaler.fit_transform(X_train)\n",
    "        scaled_X_test = scaler.transform(X_test)\n",
    "    else: \n",
    "        msg = \"Without Standardisation\"\n",
    "        scaled_X_train = X_train\n",
    "        scaled_X_test = X_test\n",
    "    print(\"Example ran {}\".format(msg))\n",
    "    # perform PCA, we use 2 so that we can plot readily.\n",
    "    print(\"PCA\")\n",
    "    n_components = 2\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(scaled_X_train)  \n",
    "    X_train_pca = pca.transform(scaled_X_train)\n",
    "    X_test_pca = pca.transform(scaled_X_test)\n",
    "    first_pca_component = pd.DataFrame(\n",
    "    pca.components_[0], index=X.columns, columns=[msg]\n",
    "    )\n",
    "    first_pca_component.plot.bar(\n",
    "    title=\"Weights of the first principal component\", figsize=(6, 8)\n",
    "    )\n",
    "    # fit a linear SVM, note we do a randomize search on C between 1 and 100.\n",
    "    # if you wish to observe rbf/poly then you need to include the gamma with sensible ranges.\n",
    "    param_grid = {\n",
    "        \"C\": loguniform(1e0, 1e2),\n",
    "        #\"gamma\": loguniform(1e-4, 1e0),\n",
    "    }\n",
    "    clf = RandomizedSearchCV(\n",
    "        SVC(kernel=\"linear\", class_weight=\"balanced\"), param_grid, n_iter=10\n",
    "    )\n",
    "    clf = clf.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # print out the coefs for the best fit\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    # plot the boundaries between classes in PC1 and PC2 space.\n",
    "    decision_plot(X_train_pca, y_train,clf)\n",
    "    y_pred = clf.predict(X_train_pca)\n",
    "    \n",
    "    # run classification reports on the training and test data\n",
    "    print(\"Training data\")\n",
    "    print(classification_report(y_train, y_pred, target_names=np.char.mod(\"Class %d\",target_classes)))\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print(\"Test data\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.char.mod(\"Class %d\",target_classes)))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32740d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2023)",
   "language": "python",
   "name": "python3-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
