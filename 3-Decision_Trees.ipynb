{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f01c187-6b2b-4fd1-9436-6ea92556b84f",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Decision trees are classifiers that parition space by forming a series of conditions. e.g. value is greater than X, class A otherwise Class B etc. etc.\n",
    "\n",
    "To test this process we create data that is an overlapping spiral using a paramatic equation. The formation isn't important but we define parameters for number of arcs, noise and gap between spiral arms, which we use to see how the classifier behaves.\n",
    "\n",
    "You can change these values and view the spiral output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea2730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6765b75-c14c-4701-94b8-97711c2864c6",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "Experiment with changing noise, test/train split and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a8d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate spiral data (see https://gist.github.com/45deg/e731d9e7f478de134def5668324c44c5)\n",
    "noise = 1.0  #  Size of the noise added.\n",
    "N = 200      #  number of samples\n",
    "split = 0.25 #  test train split i.e. 0.25 = 1/4 of the data used for testing.\n",
    "gap = 2      #  gap between spiral arms\n",
    "arcs = 1.0   #  number of rotations for sprial\n",
    "theta = np.sqrt(np.random.rand(N))*arcs*2*pi # np.linspace(0,2*pi,100)\n",
    "\n",
    "r_a = gap*theta + pi\n",
    "data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T\n",
    "x_a = data_a + noise*np.random.randn(N,2)\n",
    "\n",
    "r_b = -gap*theta - pi\n",
    "data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T\n",
    "x_b = data_b + noise*np.random.randn(N,2)\n",
    "\n",
    "res_a = np.append(x_a, np.zeros((N,1)), axis=1)\n",
    "res_b = np.append(x_b, np.ones((N,1)), axis=1)\n",
    "\n",
    "res = np.append(res_a, res_b, axis=0)\n",
    "np.random.shuffle(res)\n",
    "\n",
    "plt.scatter(x_a[:,0],x_a[:,1])\n",
    "plt.scatter(x_b[:,0],x_b[:,1])\n",
    "X=np.r_[x_a,x_b]\n",
    "y=np.r_[np.zeros(N),np.ones(N)]\n",
    "print(\"Input data\")\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y, test_size=split, random_state=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d90d06-1940-4f07-882c-660936411ec0",
   "metadata": {},
   "source": [
    "## Basic Decision Tree\n",
    "This creates a Decision Tree Classifier, which will be visualised as a set of conditional branches.\n",
    "We also see a ROC curve (see lectures) which allows us to see how well it is doing.\n",
    "\n",
    "The ROC curve is summarised with the Area Under the Curve (AUC), the closer this is to 1.0 the better the classfication.\n",
    "\n",
    "We additionally perform a 10 fold Cross Validation to assess how well the classifier will work on new data.\n",
    "\n",
    "Experiment with changing the maximum depth of the tree, how does this effect the AUC.\n",
    "Does the average AUC predicted by cross validation agree with the value from the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ab828-053b-4411-a0ab-218decbec679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_depth = None # how deep the tree generated can go, None is uncapped, experiment with lower values\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0,max_depth=max_depth)\n",
    "\n",
    "print(\"10 fold cross validation\")\n",
    "print(cross_val_score(clf, X_train,y_train, scoring='roc_auc', cv=10))\n",
    "\n",
    "print(\"Final model\")\n",
    "clf = clf.fit(X_train,y_train)\n",
    "print(\"AUC\",roc_auc_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c0134-ea1b-40a1-93de-ce260b0a84ae",
   "metadata": {},
   "source": [
    "### Decision boundary plot\n",
    "This is a helper that shows the decision boundaries for the classifier, which we apply to the above tree.\n",
    "\n",
    "How many parts does the tree have, do they all join up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4ced8-83cd-43aa-844a-26515c198984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(_X, _y, tree):\n",
    "    feature_1, feature_2 = np.meshgrid(\n",
    "        np.linspace(_X[:, 0].min(), _X[:, 0].max()),\n",
    "        np.linspace(_X[:, 1].min(), _X[:, 1].max())\n",
    "    )\n",
    "    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "    y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n",
    "    display = DecisionBoundaryDisplay(\n",
    "        xx0=feature_1, xx1=feature_2, response=y_pred\n",
    "    )\n",
    "    display.plot(alpha=0.5, cmap=\"plasma\")\n",
    "    display.ax_.scatter(\n",
    "        _X[:, 0], _X[:, 1], c=_y, edgecolor=\"black\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Decision Boundary Plot\")\n",
    "print(\"Training data\")\n",
    "plot_decision_boundaries(X_train, y_train, clf)\n",
    "print(\"Test data\")\n",
    "plot_decision_boundaries(X_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cba07a-8331-4ddc-8ad1-5ad44ab26819",
   "metadata": {},
   "source": [
    "## Bagging - Random forest\n",
    "\n",
    "Decision trees are prone to overfitting, as they keep separating data over and over again.\n",
    "To Avoid this we can create lots of decision trees and then get a consensus result from them all.\n",
    "This is known as a Random Forest.\n",
    "\n",
    "This example creates a random forest from the data, we have some hyperparameters 9such as number of estimators and maximum depth, we perform a small randomised search to pick these values using the data and a 5-fold Cross Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c7b3e-43c0-406c-a82c-caed71e1d250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter dictionary\n",
    "param_dist={\n",
    "    'n_estimators':range(50,100), # size of forest\n",
    "    'max_depth':range(1,20) # depth of forest - \"pruning\"\n",
    "}\n",
    "\n",
    "# perform a \"randomised\" grid search of hyperparameters\n",
    "print(\"Picking hyperparameters\")\n",
    "rf = RandomForestClassifier()\n",
    "rand_search=RandomizedSearchCV(rf,\n",
    "                               param_distributions=param_dist,\n",
    "                               n_iter=5, \n",
    "                               cv=5)\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# get best model and parameters\n",
    "best_rf = rand_search.best_estimator_\n",
    "print(rand_search.best_params_)\n",
    "\n",
    "# evaluate best model\n",
    "print(\"AUC\", roc_auc_score(y_test, best_rf.predict(X_test)))\n",
    "\n",
    "# print decision boundary plot.\n",
    "print(\"Decision Boundary Plot\")\n",
    "print(\"Training data\")\n",
    "plot_decision_boundaries(X_train, y_train, best_rf)\n",
    "print(\"Test data\")\n",
    "plot_decision_boundaries(X_test, y_test, best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b956a11-d927-4954-9dbe-178ece88fe66",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting creates a sequence of classifiers with a shallow decision tree (we set max_depth 4 to start with).\n",
    "The points that aren't classified by this tree are then weighted and another tree is created to better classify these.\n",
    "Again the series of trees are used to find the final result.\n",
    "\n",
    "_Note_: The spiral dataset does not benefit as much from this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e3233-95c6-4715-a3a6-cc15c08738dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(random_state=0,max_depth=max_depth), n_estimators=300, random_state=0\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"AUC\", roc_auc_score(y_test, ada_clf.predict(X_test)))\n",
    "\n",
    "# print decision boundary plot.\n",
    "print(\"Decision Boundary Plot\")\n",
    "print(\"Training data\")\n",
    "plot_decision_boundaries(X_train, y_train, ada_clf)\n",
    "print(\"Test data\")\n",
    "plot_decision_boundaries(X_test, y_test, ada_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2025)",
   "language": "python",
   "name": "python3-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
