{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.8.20","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0f01c187-6b2b-4fd1-9436-6ea92556b84f","cell_type":"markdown","source":"# Decision Trees\nDecision trees are classifiers that parition space by forming a series of conditions. e.g. value is greater than X, class A otherwise Class B etc. etc.\n\nTo test this process we create data that is an overlapping spiral using a paramatic equation. The formation isn't important but we define parameters for number of arcs, noise and gap between spiral arms, which we use to see how the classifier behaves.\n\nYou can change these values and view the spiral output.","metadata":{}},{"id":"70ea2730","cell_type":"code","source":"import numpy as np\nfrom numpy import pi\nfrom numpy.random import randint\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, RocCurveDisplay\nfrom sklearn.model_selection import cross_val_score, train_test_split,RandomizedSearchCV,GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"db01766d-c36d-49d4-af90-5f13b254939a","cell_type":"markdown","source":"## 1D data\nThe first set of data is simple 1D points, half are red and half are green, the centre and spread of the data can be controlled by the mean and sigma parameters.\n\nWe first plot of the data, then create a decision tree using all the data with an unconstrained max depth.\nWe plot the resulting tree, step of logical conditions that classifies the data, and a \"confusion matrix\" true red/green vs predicted red/green.\nWe finally plot the ROC curve with area under the curve.\n\n1. Experiment with increasing the noise, and observing how many additional branches are formed in the tree\n2. With noisy data, experiment with capping the max depth (e.g. 2-3) and observing the impact on the confusion matrix and the ROC curve.\n","metadata":{}},{"id":"da646484-7813-46c1-86de-19cf3a0f8719","cell_type":"code","source":"from matplotlib import ticker\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# parameters for the input data - centre (mean) and spread (sigma)\nN=40 # number of points\ngreen_mean = 5\ngreen_sigma = 1\nred_mean = -5\nred_sigma = 1\n\n# Print the test data\nx=np.concatenate((np.random.normal(green_mean,green_sigma,N),np.random.normal(red_mean,red_sigma,N))).reshape(-1, 1)\nlabels = np.concatenate((np.full(N,1),np.full(N,0)))\ncolors = np.concatenate((np.full(N,\"green\"),np.full(N,\"red\")))\nplt.scatter(x,np.zeros(2*N), color = colors)\nplt.show()\n\nmax_depth = None # how deep the tree generated can go, None is uncapped, experiment with lower values\n\n# create the classifier and fit\nclf = DecisionTreeClassifier(random_state=0,max_depth=max_depth)\nclf = clf.fit(x,labels)\n\nprint(\"Decision Tree\")\nplt.figure(figsize=(10,10))\nplot_tree(clf)\nplt.show()\nprint(\"Confusion Matrix\") \ncf = confusion_matrix(labels,clf.predict(x))\ndf = pd.DataFrame(cf,columns=[\"True Green\", \"True Red\"],index=[\"Pred Green\", \"Pred Red\"])\nprint(df)\n\nprint(\"ROC Curve\")\nclf_disp = RocCurveDisplay.from_estimator(clf, x, labels)\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b6765b75-c14c-4701-94b8-97711c2864c6","cell_type":"markdown","source":"### Data generator\nExperiment with changing noise, test/train split and number of samples","metadata":{}},{"id":"180a8d19","cell_type":"code","source":"# generate spiral data (see https://gist.github.com/45deg/e731d9e7f478de134def5668324c44c5)\nnoise = 1.0  #  Size of the noise added.\nN = 200      #  number of samples\nsplit = 0.25 #  test train split i.e. 0.25 = 1/4 of the data used for testing.\ngap = 2      #  gap between spiral arms\narcs = 1.0   #  number of rotations for sprial\ntheta = np.sqrt(np.random.rand(N))*arcs*2*pi # np.linspace(0,2*pi,100)\n\nr_a = gap*theta + pi\ndata_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T\nx_a = data_a + noise*np.random.randn(N,2)\n\nr_b = -gap*theta - pi\ndata_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T\nx_b = data_b + noise*np.random.randn(N,2)\n\nres_a = np.append(x_a, np.zeros((N,1)), axis=1)\nres_b = np.append(x_b, np.ones((N,1)), axis=1)\n\nres = np.append(res_a, res_b, axis=0)\nnp.random.shuffle(res)\n\nplt.scatter(x_a[:,0],x_a[:,1])\nplt.scatter(x_b[:,0],x_b[:,1])\nX=np.r_[x_a,x_b]\ny=np.r_[np.zeros(N),np.ones(N)]\nprint(\"Input data\")\nplt.show()\n\nX_train, X_test, y_train, y_test =train_test_split(X,y, test_size=split, random_state=None)\n","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"00d90d06-1940-4f07-882c-660936411ec0","cell_type":"markdown","source":"## Basic Decision Tree\nThis creates a Decision Tree Classifier, which will be visualised as a set of conditional branches.\nWe also see a ROC curve (see lectures) which allows us to see how well it is doing.\n\nThe ROC curve is summarised with the Area Under the Curve (AUC), the closer this is to 1.0 the better the classfication.\n\nWe additionally perform a 10 fold Cross Validation to assess how well the classifier will work on new data.\n\nExperiment with changing the maximum depth of the tree, how does this effect the AUC.\nDoes the average AUC predicted by cross validation agree with the value from the test data?","metadata":{}},{"id":"908ab828-053b-4411-a0ab-218decbec679","cell_type":"code","source":"max_depth = None # how deep the tree generated can go, None is uncapped, experiment with lower values\n\nclf = DecisionTreeClassifier(random_state=0,max_depth=max_depth)\n\nprint(\"10 fold cross validation\")\nprint(cross_val_score(clf, X_train,y_train, scoring='roc_auc', cv=10))\n\nprint(\"Final model\")\nclf = clf.fit(X_train,y_train)\nprint(\"AUC\",roc_auc_score(y_test, clf.predict(X_test)))\n\nclf_disp = RocCurveDisplay.from_estimator(clf, X_test, y_test)\nplt.show()\n\nprint(\"Decision Tree\")\nplt.figure(figsize=(10,10))\nplot_tree(clf)\nplt.show()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"396c0134-ea1b-40a1-93de-ce260b0a84ae","cell_type":"markdown","source":"### Decision boundary plot\nThis is a helper that shows the decision boundaries for the classifier, which we apply to the above tree.\n\nHow many parts does the tree have, do they all join up?","metadata":{}},{"id":"49e4ced8-83cd-43aa-844a-26515c198984","cell_type":"code","source":"def plot_decision_boundaries(_X, _y, tree):\n    feature_1, feature_2 = np.meshgrid(\n        np.linspace(_X[:, 0].min(), _X[:, 0].max()),\n        np.linspace(_X[:, 1].min(), _X[:, 1].max())\n    )\n    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n    display = DecisionBoundaryDisplay(\n        xx0=feature_1, xx1=feature_2, response=y_pred\n    )\n    display.plot(alpha=0.5, cmap=\"plasma\")\n    display.ax_.scatter(\n        _X[:, 0], _X[:, 1], c=_y, edgecolor=\"black\"\n    )\n    plt.show()\n    \nprint(\"Decision Boundary Plot\")\nprint(\"Training data\")\nplot_decision_boundaries(X_train, y_train, clf)\nprint(\"Test data\")\nplot_decision_boundaries(X_test, y_test, clf)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"12cba07a-8331-4ddc-8ad1-5ad44ab26819","cell_type":"markdown","source":"## Bagging - Random forest\n\nDecision trees are prone to overfitting, as they keep separating data over and over again.\nTo Avoid this we can create lots of decision trees and then get a consensus result from them all.\nThis is known as a Random Forest.\n\nThis example creates a random forest from the data, we have some hyperparameters 9such as number of estimators and maximum depth, we perform a small randomised search to pick these values using the data and a 5-fold Cross Validation.\n","metadata":{}},{"id":"886c7b3e-43c0-406c-a82c-caed71e1d250","cell_type":"code","source":"# Hyperparameter dictionary\nparam_dist={\n    'n_estimators':range(50,100), # size of forest\n    'max_depth':range(1,20) # depth of forest - \"pruning\"\n}\n\n# perform a \"randomised\" grid search of hyperparameters\nprint(\"Picking hyperparameters\")\nrf = RandomForestClassifier()\nrand_search=RandomizedSearchCV(rf,\n                               param_distributions=param_dist,\n                               n_iter=5, \n                               cv=5)\nrand_search.fit(X_train, y_train)\n\n# get best model and parameters\nbest_rf = rand_search.best_estimator_\nprint(rand_search.best_params_)\n\n# evaluate best model\nprint(\"AUC\", roc_auc_score(y_test, best_rf.predict(X_test)))\n\n# print decision boundary plot.\nprint(\"Decision Boundary Plot\")\nprint(\"Training data\")\nplot_decision_boundaries(X_train, y_train, best_rf)\nprint(\"Test data\")\nplot_decision_boundaries(X_test, y_test, best_rf)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2b956a11-d927-4954-9dbe-178ece88fe66","cell_type":"markdown","source":"## Boosting\n\nBoosting creates a sequence of classifiers with a shallow decision tree (we set max_depth 4 to start with).\nThe points that aren't classified by this tree are then weighted and another tree is created to better classify these.\nAgain the series of trees are used to find the final result.\n\n_Note_: The spiral dataset does not benefit as much from this approach.\n","metadata":{}},{"id":"979e3233-95c6-4715-a3a6-cc15c08738dd","cell_type":"code","source":"max_depth = 4\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(random_state=0,max_depth=max_depth), n_estimators=300, random_state=0\n)\n\nada_clf.fit(X_train, y_train)\n\nprint(\"AUC\", roc_auc_score(y_test, ada_clf.predict(X_test)))\n\n# print decision boundary plot.\nprint(\"Decision Boundary Plot\")\nprint(\"Training data\")\nplot_decision_boundaries(X_train, y_train, ada_clf)\nprint(\"Test data\")\nplot_decision_boundaries(X_test, y_test, ada_clf)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}